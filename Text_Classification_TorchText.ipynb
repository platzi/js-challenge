{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtrochepy/js-challenge/blob/main/Text_Classification_TorchText.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rQEo56DjYQ8"
      },
      "source": [
        "\"Neural Networks Course with PyTorch\n",
        "Instructor: Omar Uriel Espejel Diaz **texto en negrita**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KztstNvjdNt"
      },
      "source": [
        "**Text Classification with TorchText**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf2vCKdYjnLE"
      },
      "source": [
        "The PyTorch Project contains libraries for different types of data and purposes.\n",
        "\n",
        "torchaudio\n",
        "torchvision\n",
        "TorchElastic\n",
        "TorchServe **texto en negrita**\n",
        "\n",
        "We will use torchtext for text classification. The torchtext package consists of data processing utilities and popular datasets for natural language processing.\n",
        "\n",
        "However, feel free to try other available libraries in PyTorch. torchvision is particularly used for applications working with images!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDno-pKaju1k"
      },
      "source": [
        "**1. Importing libraries and dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eUZT150Kjt3c"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install portalocker>=2.0.0\n",
        "!pip install torchtext --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Ab1Kmyvj7P7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "from torchtext.datasets import DBpedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlshADFQkA0z"
      },
      "source": [
        "**Check the version**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxnt82eykCJz",
        "outputId": "294ada42-bbac-469f-e4cf-32c8820d3f7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'torchtext.version' from '/usr/local/lib/python3.10/dist-packages/torchtext/version.py'>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "torchtext.version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuXHbtqLkYDT"
      },
      "source": [
        "**2. Processing the dataset and creating a vocabulary**\n",
        "\n",
        "Import the torch and torchtext libraries. Use torchtext to load the DBpedia dataset.\n",
        "\n",
        "Next, use the iter function to create an iteration object for the training dataset. Finally, the code prints the version of the torchtext library used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5AG5HdjmkdUU"
      },
      "outputs": [],
      "source": [
        "train_iter = iter(DBpedia(split=\"train\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BLLJrlmkfzi",
        "outputId": "cdb51b82-7e7b-4b03-d314-392c48817474"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'E. D. Abbott Ltd  Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "next(train_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hph2jCiYkmBT"
      },
      "source": [
        "We will build a vocabulary with the dataset by implementing the built-in function **build_vocab_from_iterator**, which accepts the iterator that produces a list or iterator of tokens.\n",
        "\n",
        "We use **torchtext** to build a vocabulary from an English DBpedia dataset.\n",
        "\n",
        "First, import the **get_tokenizer** function from the **torchtext** library to get a pre-defined tokenizer for the English language. Then, define a data iterator for the training dataset of DBpedia.\n",
        "\n",
        "Next, define a **yield_tokens** function that uses the tokenizer to split the text into tokens and yield them one by one. This function is used as input to the **build_vocab_from_iterator** function, which builds a vocabulary from the tokens returned by the **yield_tokens** function. The **build_vocab_from_iterator** function also takes a list of special tokens, which will be used to represent out-of-vocabulary words.\n",
        "\n",
        "In summary, this code snippet builds a vocabulary from a training dataset and prepares it for use in machine learning models using PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "coc83eMwk-1X"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "train_iter = DBpedia(split=\"train\")\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ltib9ka-lPys"
      },
      "source": [
        "Our vocabulary converts the list of tokens into integers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OnPO2n-lTfF",
        "outputId": "d7140473-33d3-49f8-b9a8-02e7df3f60a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7296, 1506, 47, 578, 2323, 187, 2409, 5, 0, 1078]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "vocab(tokenizer(\"Hello how are you? I am a platzi student\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSR4gVmPlY9T"
      },
      "source": [
        "Define two lambda functions, **text_pipeline** and **label_pipeline**, which are used to process the input data into a format that can be used for training and evaluating models.\n",
        "\n",
        "The first function, **text_pipeline**, takes a text string as input and processes it using the tokenizer and vocabulary we defined. Remember that the tokenizer splits the text into tokens (words or subwords), while the vocabulary maps each token to a unique integer index. The function returns a list of integers representing the tokens in the text.\n",
        "\n",
        "The second function, **label_pipeline**, takes a label as input and converts it to an integer. In this case, the label is subtracted by **1** to adjust it to an index range of **0** to **n-1**, where **n** is the number of classes in the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3RDh8kgQlu9O"
      },
      "outputs": [],
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oISdV-eLlyHT",
        "outputId": "20284a3e-d9dd-4e6a-ad62-e8aa81d6ea06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "text_pipeline(\"Hello I am Ruben Dario\")\n",
        "label_pipeline(\"1\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZSiLkpLmMXE"
      },
      "source": [
        "Create a function called **collate_batch** to process a batch of data. The input batch is a list of tuples, where each tuple contains a label and its corresponding text.\n",
        "\n",
        "* Three lists are initialized: **label_list**, **text_list**, and **offsets**. Offsets store the starting index of each text sequence in the concatenated tensor of text sequences. It helps to keep track of the boundaries of individual text sequences within the concatenated tensor. It starts with a value of 0, representing the starting index of the first text sequence.\n",
        "\n",
        "* The function iterates over each data point in the batch. For each data point, it processes the label using **label_pipeline(_label)** and adds the result to **label_list**. It processes the text using **text_pipeline(_text)** and converts it to a torch tensor of type **torch.int64**. The processed text is added to **text_list**, and its length **(size(0))** is added to **offsets**.\n",
        "\n",
        "* The last element in the **offsets** list is removed using the slicing **offsets[:-1]**. Then, the **cumsum** function calculates the cumulative sum of the elements in the **offsets** list along dimension 0.\n",
        "\n",
        "* The **text_list** is concatenated into a single 1D tensor using **torch.cat(text_list)**.\n",
        "\n",
        "The **label_list**, **text_list**, and **offsets** tensors are converted to the specified device (either GPU or CPU)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "zVhEYZbWngHk"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "  label_list = []\n",
        "  text_list = []\n",
        "  offsets = [0]\n",
        "\n",
        "  for (_label, _text) in batch:\n",
        "    label_list.append(label_pipeline(_label))\n",
        "    processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "    offsets.append(processed_text.size(0))\n",
        "\n",
        "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "  text_list = torch.cat(text_list)\n",
        "  return label_list.to(device), text_list.to(device), offsets.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzAQ87IdoEMs"
      },
      "source": [
        "A **DataLoader** handles the process of iteration through a dataset in mini-batches. The DataLoader is important because it efficiently manages memory, shuffles data, and easily parallelizes data loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4nldhbAqoMbD"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_iter = DBpedia(split=\"train\")\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhDqskiSoPXU",
        "outputId": "f8115ead-8f07-4271-8672-ce7a6d27d2fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7d5a1890de10>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3WWwDGXoUWs"
      },
      "source": [
        "**3. Creating the classification model and its layers**\n",
        "\n",
        "We create **TextClassificationModel**, a neural network class that implements a simple but effective architecture for text classification, using embedding layers, batch normalization, and fully connected layers.\n",
        "\n",
        "* __init__(**self**, **vocab_size**, **embed_dim**, **num_class**): This method initializes the model with three arguments: the vocabulary size (vocab_size), the embedding dimension (embed_dim), and the number of classes (num_class).\n",
        "\n",
        "* **self.embedding**: The embedding layer (nn.EmbeddingBag) converts each word in the text into a vector of dimension embed_dim. The embedding is efficiently performed in batches for the text sequences in the input.\n",
        "\n",
        "* **self.bn1**: The batch normalization layer (nn.BatchNorm1d) improves the stability and training speed of the model by normalizing the input features along the specified dimension (in this case, embed_dim).\n",
        "\n",
        "**self.fc**: The fully connected layer (nn.Linear) performs theSure! I can help you with the translation and improvement of the documentation for the given PyTorch Neural Network model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MYit29tSqIDi"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "  def __init__(self, vocab_size, embed_dim, num_class):\n",
        "    super(TextClassificationModel, self).__init__()\n",
        "\n",
        "    # Embedding layer\n",
        "    self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
        "\n",
        "    # Batch normalization layer\n",
        "    self.bn1 = nn.BatchNorm1d(embed_dim)\n",
        "\n",
        "    # Fully connected layer\n",
        "    self.fc = nn.Linear(embed_dim, num_class)\n",
        "\n",
        "  def forward(self, text, offsets):\n",
        "    # Embed the text\n",
        "    embedded = self.embedding(text, offsets)\n",
        "\n",
        "    # Apply batch normalization\n",
        "    embedded_norm = self.bn1(embedded)\n",
        "\n",
        "    # Apply the ReLU activation function\n",
        "    embedded_activated = F.relu(embedded_norm)\n",
        "\n",
        "    # Output the class probabilities\n",
        "    return self.fc(embedded_activated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CUS-ZhsqMEb"
      },
      "source": [
        "We build a model with an embedding dimension of 100."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABs82WC6qPUM",
        "outputId": "908e09f2-8444-4605-99ec-74683a30c234"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 80,301,414 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "train_iter = DBpedia(split=\"train\")\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 100\n",
        "\n",
        "model = TextClassificationModel(vocab_size=vocab_size, embed_dim=embedding_size, num_class=num_class).to(device)\n",
        "\n",
        "# Model architecture\n",
        "# print(model)\n",
        "\n",
        "# Number of trainable parameters in our model\n",
        "def count_parameters(model):\n",
        "  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_5BxeUprHbT"
      },
      "source": [
        "**4. Functions for Model Training and Evaluation**\n",
        "\n",
        "We now define the functions to train the model and evaluate the results.\n",
        "\n",
        "We use **torch.nn.utils.clip_grad_norm_** to limit the maximum value of the gradient norm during the training of a neural network. In other words, it ensures that the gradients aren't too large, and thus avoids the neural network becoming unstable during training.\n",
        "\n",
        "The first argument, **model.parameters()**, refers to the parameters of the model being trained. The second argument, \"0.1\", is the maximum allowed value for the gradient norm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l13wBehQrRZD"
      },
      "outputs": [],
      "source": [
        "def train(dataloader):\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Initialize accuracy, count, and loss for each epoch\n",
        "    epoch_acc = 0\n",
        "    epoch_loss = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        # Reset gradients after each batch\n",
        "        optimizer.zero_grad()\n",
        "        # Get model predictions\n",
        "        prediction = model(text, offsets)\n",
        "\n",
        "        # Get the loss\n",
        "        loss = criterion(prediction, label)\n",
        "\n",
        "        # Backpropagate the loss and compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # Get the accuracy\n",
        "        acc = (prediction.argmax(1) == label).sum()\n",
        "\n",
        "        # Prevent gradients from becoming too large\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "\n",
        "        # Update the weights\n",
        "        optimizer.step()\n",
        "\n",
        "        # Keep track of the loss and accuracy for this epoch\n",
        "        epoch_acc += acc.item()\n",
        "        epoch_loss += loss.item()\n",
        "        total_count += label.size(0)\n",
        "\n",
        "        if idx % 500 == 0 and idx > 0:\n",
        "            print(f\" epoch {epoch} | {idx}/{len(dataloader)} batches | loss {epoch_loss/total_count} | accuracy {epoch_acc/total_count}\")\n",
        "\n",
        "    return epoch_acc/total_count, epoch_loss/total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "qzk1h2mBrUo0"
      },
      "outputs": [],
      "source": [
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    epoch_acc = 0\n",
        "    total_count = 0\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            # Get the predicted label\n",
        "            prediction = model(text, offsets)\n",
        "\n",
        "            # Get loss and accuracy\n",
        "            loss = criterion(prediction, label)\n",
        "            acc = (prediction.argmax(1) == label).sum()\n",
        "\n",
        "            # Keep track of the loss and accuracy for this epoch\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "            total_count += label.size(0)\n",
        "\n",
        "    return epoch_acc/total_count, epoch_loss/total_count\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QK-R4h6mrd2j"
      },
      "source": [
        "Please note that the variable **epoch** inside the training function is not defined within the function scope. Make sure it is defined globally or passed as an argument if you are running epochs outside the function scope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXMCx_3QsCoU"
      },
      "source": [
        "**5. Preparing for Training: Data Split, Loss, and Optimization**\n",
        "\n",
        "We split the training dataset into valid training sets with a split ratio of 0.95 (training) and 0.5 (valid) using the function torch.utils.data.dataset.random_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5zdsM9JsJAs"
      },
      "source": [
        "**Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JurUAn81sMyL"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 4 # epochs\n",
        "LEARNING_RATE = 0.2 # learning rate\n",
        "BATCH_SIZE = 64 # batch size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yW-47t5LsSVb"
      },
      "source": [
        "Explore the other loss functions available in PyTorch. You can find them all here: https://pytorch.org/docs/stable/nn.html#loss-functions.\n",
        "\n",
        "The loss function is the one that measures how good our model's predictions are compared to the actual labels. PyTorch offers a wide range of loss functions that we can use to train our models on different types of problems, such as regression, classification, and sequence-to-sequence modeling.\n",
        "\n",
        "By delving into these other loss functions, we can expand our machine learning knowledge. The same applies to the optimizers. PyTorch provides a variety of optimization algorithms: https://pytorch.org/docs/stable/optim.html#algorithms.\n",
        "\n",
        "Spend time exploring PyTorch's documentation on loss functions and optimizers. Experiment with different functions in your projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjHyWpx4sWv0"
      },
      "source": [
        "**Loss, Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "i7ZECcVYsZoz"
      },
      "outputs": [],
      "source": [
        "#Loss, Optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr= LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9Cgbe6Nspzz"
      },
      "source": [
        "We split the dataset into three parts: training, validation, and test.\n",
        "\n",
        "First, we import the **random_split** function from the Dataset class and the **to_map_style_dataset** function from **torchtext.data.functional**. Then, we load the **DBpedia** dataset using the **DBpedia()** method. Next, we convert the dataset into a format that can be used by PyTorch's **DataLoader** using the **to_map_style_dataset** function.\n",
        "\n",
        "We then define the proportion of data we will use to train our model (95%) and the percentage we will use to validate our model (5%). We use the **random_split** function to split the training dataset into training and validation.\n",
        "\n",
        "Finally, we create three DataLoaders for each part of the dataset: one for training, one for validation, and another for testing. We use the **batch_size** argument to define the size of the data batches that will be used in training and testing. The **collate_fn** argument specifies how data samples should be joined to form a batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3MYhOux5tHQc"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "# Get the trainset and testset\n",
        "train_iter, test_iter = DBpedia()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "# We train the model with 95% of the data from the trainset\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "\n",
        "# We create a validation dataset with 5% of the trainset\n",
        "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "# We create dataloaders ready to feed into our model\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt8enSSgtNkb",
        "outputId": "58d103a0-51d1-430b-9dc2-32009239d031"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.data.functional.to_map_style_dataset.<locals>._MapStyleDataset at 0x7d5a0bc48df0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "train_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvNOaM6ptS5U"
      },
      "source": [
        "This documentation serves as a comprehensive guide for setting up the model's training process. It covers the data split, selection of hyperparameters, choice of loss function and optimizer, and the creation of DataLoaders for the train, validation, and test datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2suF2MWt2AF"
      },
      "source": [
        "**6. Training and Evaluating the Model**\n",
        "\n",
        "Now, let's proceed with the training and evaluation of our model. Firstly, we define the variable **best_validation_loss** and initialize it with positive infinity. This variable is used to track the best validation loss during the training.\n",
        "\n",
        "Then, we perform a **for** loop across epochs. Within each epoch, the model's training and validation is performed using the respective training and validation datasets.\n",
        "\n",
        "In other words, if the current validation loss is less than the previous best validation loss, we save the current state of the model in the **saved_weights.pt** file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3MMUZBxuDgb",
        "outputId": "48baa734-f690-4ddb-99b7-6b50a02b0bd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " epoch 1 | 500/8313 batches | loss 0.033213432907998684 | accuracy 0.38566616766467066\n",
            " epoch 1 | 1000/8313 batches | loss 0.02894332105448315 | accuracy 0.4741976773226773\n",
            " epoch 1 | 1500/8313 batches | loss 0.02638594473064998 | accuracy 0.5161142571618921\n",
            " epoch 1 | 2000/8313 batches | loss 0.024672556727163973 | accuracy 0.5444621439280359\n",
            " epoch 1 | 2500/8313 batches | loss 0.023443429099648343 | accuracy 0.563811975209916\n",
            " epoch 1 | 3000/8313 batches | loss 0.022505414862432745 | accuracy 0.5778907030989671\n",
            " epoch 1 | 3500/8313 batches | loss 0.021759408591030514 | accuracy 0.5892780634104542\n",
            " epoch 1 | 4000/8313 batches | loss 0.02117373248558958 | accuracy 0.5981551487128218\n",
            " epoch 1 | 4500/8313 batches | loss 0.020674863539286863 | accuracy 0.6062819373472562\n",
            " epoch 1 | 5000/8313 batches | loss 0.020251466184064905 | accuracy 0.6128430563887223\n",
            " epoch 1 | 5500/8313 batches | loss 0.01985594714976342 | accuracy 0.6191743546627886\n",
            " epoch 1 | 6000/8313 batches | loss 0.019518162545056974 | accuracy 0.6246927595400766\n",
            " epoch 1 | 6500/8313 batches | loss 0.019208465915427145 | accuracy 0.6296579372404245\n",
            " epoch 1 | 7000/8313 batches | loss 0.01892563849160789 | accuracy 0.6343200971289815\n",
            " epoch 1 | 7500/8313 batches | loss 0.018686322826847936 | accuracy 0.6382648980135982\n",
            " epoch 1 | 8000/8313 batches | loss 0.018447154393162843 | accuracy 0.6424138388951381\n",
            " epoch 2 | 500/8313 batches | loss 0.014640667302879745 | accuracy 0.7084892714570858\n",
            " epoch 2 | 1000/8313 batches | loss 0.014507226681301406 | accuracy 0.7108204295704296\n",
            " epoch 2 | 1500/8313 batches | loss 0.014467753206555304 | accuracy 0.7110363924050633\n",
            " epoch 2 | 2000/8313 batches | loss 0.014386769824673882 | accuracy 0.7126046351824088\n",
            " epoch 2 | 2500/8313 batches | loss 0.01428652925585143 | accuracy 0.7145329368252699\n",
            " epoch 2 | 3000/8313 batches | loss 0.014219103023373845 | accuracy 0.7157197600799733\n",
            " epoch 2 | 3500/8313 batches | loss 0.014147156351654274 | accuracy 0.7171076121108255\n",
            " epoch 2 | 4000/8313 batches | loss 0.01408576367118472 | accuracy 0.7183165146213447\n",
            " epoch 2 | 4500/8313 batches | loss 0.014026052809004432 | accuracy 0.7195241335258832\n",
            " epoch 2 | 5000/8313 batches | loss 0.01393925337375277 | accuracy 0.7211463957208558\n",
            " epoch 2 | 5500/8313 batches | loss 0.01386453512891141 | accuracy 0.7227237093255772\n",
            " epoch 2 | 6000/8313 batches | loss 0.01381938635579431 | accuracy 0.723756978003666\n",
            " epoch 2 | 6500/8313 batches | loss 0.013773329550186349 | accuracy 0.724633710198431\n",
            " epoch 2 | 7000/8313 batches | loss 0.01371077283967588 | accuracy 0.7257601592629624\n"
          ]
        }
      ],
      "source": [
        "# Obtain the best loss\n",
        "best_validation_loss = float('inf')\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    # Training\n",
        "    train_acc, train_loss = train(train_dataloader)\n",
        "\n",
        "    # Validation\n",
        "    validation_acc, validation_loss = evaluate(valid_dataloader)\n",
        "\n",
        "    # Save the best model\n",
        "    if validation_loss < best_validation_loss:\n",
        "      best_validation_loss = validation_loss\n",
        "      torch.save(model.state_dict(), \"best_saved.pt\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGsDOaddoxrRpm1yAebTYD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}